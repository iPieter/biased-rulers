{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_file\n",
    "male_list = data_file.male_list\n",
    "female_list = data_file.female_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gendered_word_list\n",
    "# gendered_word_list = [[\"she\", \"he\"],[\"female\", \"male\"], [\"woman\", \"man\"], [\"girl\", \"boy\"], [\"sister\", \"brother\"], [\"daughter\", \"son\"], [\"mother\", \"father\"]]\n",
    "XX = [\"female\", \"woman\", \"girl\", \"sister\", \"daughter\", \"mother\", \"aunt\", \"grandmother\", \"daughter\"]\n",
    "YY = [\"male\", \"man\", \"boy\", \"brother\", \"son\", \"father\", \"uncle\", \"grandfather\", \"son\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['male',\n",
       " 'man',\n",
       " 'boy',\n",
       " 'brother',\n",
       " 'son',\n",
       " 'father',\n",
       " 'uncle',\n",
       " 'grandfather',\n",
       " 'son']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\"this is the _.\", \"that is the _.\", \"there is the _.\", \"here is the _.\", \"the _ is here.\", \"the _ is there.\", \"the _ is a person.\", \"it is the _.\",\"the _ is a [MASK].\", \"the _ is an engineer.\", \"the _ is a nurse with superior technical skills.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [\"temp1\", \"temp2\", \"temp3\", \"temp4\",\"temp5\", \"temp6\", \"temp7\", \"temp8\", \"temp9\", \"temp10\", \"temp11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(sentence, word):\n",
    "    toks = tokenizer(sentence).input_ids\n",
    "    wordpieces = tokenizer(word).input_ids\n",
    "#     print(toks)\n",
    "    word = wordpieces[1] # use first wordpiece\n",
    "    for i, t in enumerate(toks):\n",
    "        if t == word:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS embedding\n",
    "def sentence_embedding(template, word):\n",
    "    sentence = template.replace(\"_\", word)\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = last_hidden_states\n",
    "    return token_embeddings[0][0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(x, y):\n",
    "    return np.dot(x, y) / math.sqrt(np.dot(x, x) * np.dot(y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cossim_lookup(XY, AB):\n",
    "    \"\"\"\n",
    "    XY: mapping from target string to target vector (either in X or Y)\n",
    "    AB: mapping from attribute string to attribute vectore (either in A or B)\n",
    "    Returns an array of size (len(XY), len(AB)) containing cosine similarities\n",
    "    between items in XY and items in AB.\n",
    "    \"\"\"\n",
    "\n",
    "    cossims = np.zeros((len(XY), len(AB)))\n",
    "    for xy in XY:\n",
    "        for ab in AB:\n",
    "            cossims[xy, ab] = cossim(XY[xy], AB[ab])\n",
    "    return cossims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keys_to_ints(X, Y):\n",
    "    return (\n",
    "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
    "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def s_XAB(A, s_wAB_memo):\n",
    "    return s_wAB_memo[A].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_wAB(X, Y, cossims):\n",
    "    \"\"\"\n",
    "    Return vector of s(w, A, B) across w, where\n",
    "        s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "    \"\"\"\n",
    "#     print((cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)).shape)\n",
    "    return cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def s_wAB_df(X, Y, cossims):\n",
    "#     \"\"\"\n",
    "#     Return vector of s(w, A, B) across w, where\n",
    "#         s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "#     \"\"\"\n",
    "# #     print(cossims[:, A].mean(axis=1))\n",
    "#     df1 = pd.DataFrame(cossims[X, :].mean(axis=0))\n",
    "#     df2 = pd.DataFrame(cossims[Y, :].mean(axis=0))\n",
    "#     return df1, df2\n",
    "\n",
    "\n",
    "def s_XAB_df(A, B, s_wAB_memo):\n",
    "    df1 = pd.DataFrame(s_wAB_memo[A])\n",
    "    df2 = pd.DataFrame(s_wAB_memo[B])\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_XYAB(A, B, s_wAB_memo):\n",
    "    r\"\"\"\n",
    "    Given indices of target concept X and precomputed s_wAB values,\n",
    "    the WEAT test statistic for p-value computation.\n",
    "    \"\"\"\n",
    "    return s_XAB(A, s_wAB_memo) - s_XAB(B, s_wAB_memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def WEAT_test(X, Y, A, B, n_samples, cossims, parametric=False):\n",
    "def WEAT_test(X, Y, A, B, n_samples, cossims):\n",
    "    ''' Compute the p-val for the permutation test, which is defined as\n",
    "        the probability that a random even partition X_i, Y_i of X u Y\n",
    "        satisfies P[s(X_i, Y_i, A, B) > s(X, Y, A, B)]\n",
    "    '''\n",
    "    X = np.array(list(X), dtype=np.int)\n",
    "    Y = np.array(list(Y), dtype=np.int)\n",
    "    A = np.array(list(A), dtype=np.int)\n",
    "    B = np.array(list(B), dtype=np.int)\n",
    "\n",
    "    assert len(X) == len(Y)\n",
    "    size = len(X)\n",
    "    s_wAB_memo = s_wAB(X, Y, cossims=cossims)\n",
    "#     print(s_wAB_memo)\n",
    "    XY = np.concatenate((X, Y))\n",
    "\n",
    "#     if parametric:\n",
    "#     log.info('Using parametric test')\n",
    "    s = s_XYAB(A, B, s_wAB_memo)\n",
    "    return s\n",
    "\n",
    "\n",
    "def convert_keys_to_ints(X, Y):\n",
    "    return (\n",
    "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
    "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(df1, df2, k=0):\n",
    "    diff = (df1[k].mean() - df2[k].mean())\n",
    "    std_ = pd.concat([df1, df2], axis=0)[k].std() + 1e-8\n",
    "    return diff / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2183888162528902\n",
      "1.149415303462839\n",
      "1.087691148849411\n",
      "1.177525663828664\n",
      "1.4163164173814078\n",
      "1.2950823985971622\n",
      "1.26394475144944\n",
      "1.0514220372148693\n",
      "1.2145630053863175\n",
      "1.2051310668137427\n",
      "1.084822541111828\n",
      "1.0279503178108953\n",
      "1.236813052322568\n",
      "0.8492021774860143\n",
      "1.2921773761447448\n",
      "1.1385837018024667\n",
      "1.1856982827980402\n",
      "1.0455117791192958\n",
      "1.2339127500000027\n",
      "1.2185111464976937\n",
      "1.0853378066411714\n",
      "0.9010622604331263\n",
      "1.2870922616015028\n",
      "1.2109165765618517\n",
      "1.3551520146823806\n",
      "1.32929939019638\n",
      "1.1852843109101585\n",
      "1.1686353477985958\n",
      "0.8076340914485503\n",
      "1.093693414899261\n",
      "1.051167217016259\n",
      "1.0027930103512999\n",
      "1.2342107498476742\n",
      "0.8747045058130719\n",
      "1.3336289351988424\n",
      "1.0206366972396446\n",
      "1.0606982209372366\n",
      "1.043784223645748\n",
      "1.005573918413763\n",
      "1.1309065010819188\n",
      "1.1669763766530812\n",
      "0.4181680738317021\n",
      "0.4233535657053087\n",
      "1.363037470444357\n",
      "0.8739863751143749\n",
      "0.6564722356270911\n",
      "0.4567167840764625\n",
      "0.5309601570159286\n",
      "0.35611418144969714\n",
      "0.5420311359164848\n",
      "-0.09242134630229705\n",
      "0.6543287714140602\n",
      "0.7533059750563742\n",
      "0.60214050868627\n",
      "0.336567487148753\n",
      "0.3722330053222867\n",
      "0.9819247116691158\n",
      "0.28762083557278445\n",
      "0.7730869663655694\n",
      "1.0057993313931815\n",
      "1.1071017826264573\n",
      "1.0472819419591701\n",
      "1.3403842344896217\n",
      "1.0382814483124652\n",
      "1.5043112458006098\n",
      "1.4238381018868056\n",
      "1.0508521503982533\n",
      "1.2197770424396275\n",
      "1.0403071570302955\n",
      "1.153410520829123\n",
      "0.8872682031343228\n",
      "1.0744639458833614\n",
      "1.2010292089926773\n",
      "0.8020044427076674\n",
      "1.5118000502637015\n",
      "1.2072248922557713\n",
      "1.173956014273342\n",
      "1.0128809460774477\n",
      "1.0923048982766221\n",
      "1.1874925608409248\n",
      "1.1026386532258445\n",
      "0.9599207565434964\n",
      "1.2791559209681864\n",
      "0.9233774950123914\n",
      "1.3614833700709126\n",
      "1.1559104845967225\n",
      "1.046693757927999\n",
      "1.024839935319404\n",
      "1.0363921635108932\n",
      "0.9653655428856865\n",
      "1.0638453731754465\n",
      "0.9771870755721002\n",
      "1.1880904268554107\n",
      "0.9828079795693851\n",
      "1.4526859837747177\n",
      "1.0281311446985228\n",
      "0.9570523390865292\n",
      "0.8114941802574163\n",
      "0.9266720392207148\n",
      "0.7300758451697971\n",
      "1.2613877355396592\n",
      "0.9572707315243716\n",
      "1.1557629731783141\n",
      "1.1353873808717931\n",
      "1.2108317113952092\n",
      "1.3672117130606638\n",
      "1.2663879617682048\n",
      "1.2071918883792665\n",
      "1.2195080615005842\n",
      "1.1499307549567737\n",
      "1.0837459121685178\n",
      "1.094656527905938\n",
      "1.2293924413172426\n",
      "0.9836870529327342\n",
      "1.4887226422774238\n",
      "1.2436407877926259\n",
      "0.9650464690124462\n",
      "0.8556480645854893\n",
      "1.0289315802759196\n",
      "0.8950748729388558\n",
      "0.3308814460102224\n",
      "0.5396912126785485\n",
      "0.7492803275404113\n",
      "0.37037680655749433\n",
      "0.891950347981051\n",
      "-0.19317593831780752\n",
      "0.3853332782880414\n",
      "0.778923726352035\n",
      "0.3988319391564771\n",
      "0.691227496979281\n",
      "0.6646424952377148\n",
      "0.8528250633845725\n",
      "0.9423202852495862\n",
      "0.8260373934195319\n",
      "0.38445087437882686\n",
      "0.6320078237438186\n",
      "-0.17419298448033738\n",
      "0.7039406868645524\n",
      "0.706907344715062\n",
      "0.4984741952488634\n",
      "1.472820224268098\n",
      "0.9519931189714741\n",
      "1.2592760613147482\n",
      "1.1904376895319317\n",
      "1.198363014780542\n",
      "1.6071631848082522\n",
      "0.866396987603062\n",
      "1.1898313808743797\n",
      "1.145155539801552\n",
      "1.167226129010816\n",
      "0.9534307440978431\n",
      "1.0696618358557255\n",
      "1.1460080857364114\n",
      "0.8586924487823557\n",
      "1.3694887277806953\n",
      "1.3269597491636196\n",
      "1.2980647058703927\n",
      "0.8196788079741166\n",
      "1.349903879918506\n",
      "1.1561828392690219\n",
      "0.9233559188444029\n",
      "0.8571426514490647\n",
      "1.3978385658138932\n",
      "0.8903742411690445\n",
      "1.2408774567262717\n",
      "0.8823992158636443\n",
      "0.79045901198237\n",
      "1.1566551240126854\n",
      "0.5688293032618007\n",
      "1.3346154892886597\n",
      "0.817952274613384\n",
      "1.0322510332409165\n",
      "1.0409687312963787\n",
      "1.1317258901928422\n",
      "1.2035917812126402\n",
      "1.1077940229111856\n",
      "0.7847221657790258\n",
      "1.0865858018911896\n",
      "0.8564096869366181\n",
      "1.1609879581181026\n",
      "0.5076615392246746\n",
      "0.34464654431094177\n",
      "0.4255167130921817\n",
      "0.16082427480055012\n",
      "0.5557726517943868\n",
      "0.03623298468883851\n",
      "-0.5143488111149218\n",
      "0.11807637191175402\n",
      "0.2288567403789077\n",
      "0.3491965595124076\n",
      "0.09999040624236111\n",
      "0.6127607108803128\n",
      "0.285814876080786\n",
      "-0.009967237545950003\n",
      "0.009295459695714657\n",
      "0.1739217770513582\n",
      "0.3523520889917352\n",
      "0.41700618234883396\n",
      "0.586223397246036\n",
      "0.5700410997150935\n",
      "0.24825187586256198\n",
      "0.6712910804675923\n",
      "0.9655569183892646\n",
      "-0.1304974127658009\n",
      "1.0459741694111655\n",
      "-0.027688435460230545\n",
      "0.5307493636530672\n",
      "0.7283600213047362\n",
      "0.9376685423448655\n",
      "0.32455020379780297\n",
      "0.47576140090552727\n",
      "0.9071386665335895\n",
      "0.5450209912092954\n",
      "0.3519064797580458\n",
      "1.0499491766553595\n",
      "0.6961892520915814\n",
      "0.1438838278783486\n",
      "0.3218740274575531\n",
      "0.6349150296358679\n",
      "0.7804787858886425\n"
     ]
    }
   ],
   "source": [
    "template_score_dict = {}\n",
    "for ind, template in enumerate(templates):\n",
    "    score_dict = {}\n",
    "    attribute_template = template\n",
    "    target_template = template\n",
    "#     for model_ind in range(len(models)):\n",
    "#     for model_ind in [2]:\n",
    "    X = {\"x\" + str(j): sentence_embedding(attribute_template, j) for j in XX}\n",
    "    Y = {\"y\" + str(j): sentence_embedding(attribute_template, j) for j in YY}\n",
    "    (X, Y) = convert_keys_to_ints(X, Y)\n",
    "    XY = X.copy()\n",
    "    XY.update(Y)\n",
    "    X = np.array(list(X), dtype=np.int)\n",
    "    Y = np.array(list(Y), dtype=np.int)\n",
    "    for i in range(len(female_list)):\n",
    "        AA = female_list[i]\n",
    "    #     print(AA)\n",
    "    #     print(XX)\n",
    "        BB = male_list[i]\n",
    "\n",
    "        A = {\"a\" + str(j): sentence_embedding(target_template, j) for j in AA}\n",
    "        B = {\"b\" + str(j): sentence_embedding(target_template, j) for j in BB}\n",
    "\n",
    "        (A, B) = convert_keys_to_ints(A, B)\n",
    "\n",
    "\n",
    "        AB = A.copy()\n",
    "        AB.update(B)\n",
    "\n",
    "        cossims = construct_cossim_lookup(XY, AB)\n",
    "        A = np.array(list(A), dtype=np.int)\n",
    "        B = np.array(list(B), dtype=np.int)\n",
    "\n",
    "\n",
    "        s_wAB_memo = s_wAB(X, Y, cossims=cossims)\n",
    "        df1,df2 = s_XAB_df(A, B, s_wAB_memo)\n",
    "        effect_size = get_effect_size(df1, df2)\n",
    "        score_dict[i] = effect_size\n",
    "        print(score_dict[i])\n",
    "    template_score_dict[temps[ind]] = score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a_file = open(\"1_seat_CLS_embeddings.json\", \"w\")\n",
    "json.dump(template_score_dict, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
