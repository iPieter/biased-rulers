{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_file\n",
    "male_list = data_file.male_list\n",
    "female_list = data_file.female_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gendered_word_list\n",
    "# gendered_word_list = [[\"she\", \"he\"],[\"female\", \"male\"], [\"woman\", \"man\"], [\"girl\", \"boy\"], [\"sister\", \"brother\"], [\"daughter\", \"son\"], [\"mother\", \"father\"]]\n",
    "XX = [\"female\", \"woman\", \"girl\", \"sister\", \"daughter\", \"mother\", \"aunt\", \"grandmother\", \"daughter\"]\n",
    "YY = [\"male\", \"man\", \"boy\", \"brother\", \"son\", \"father\", \"uncle\", \"grandfather\", \"son\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\"this is the _.\", \"that is the _.\", \"there is the _.\", \"here is the _.\", \"the _ is here.\", \"the _ is there.\", \"the _ is a person.\", \"it is the _.\",\"the _ is a [MASK].\", \"the _ is an engineer.\", \"the _ is a nurse with superior technical skills.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [\"temp1\", \"temp2\", \"temp3\", \"temp4\",\"temp5\", \"temp6\", \"temp7\", \"temp8\", \"temp9\", \"temp10\", \"temp11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(sentence, wordd, tokenizer):\n",
    "#     if tokenizer == tokenizers[2]:\n",
    "#         wordd= \" \"+wordd\n",
    "    toks = tokenizer(sentence).input_ids\n",
    "    wordpieces = tokenizer(wordd).input_ids\n",
    "    word = wordpieces[1] # use first wordpiece\n",
    "    for i, t in enumerate(toks):\n",
    "        if t == word:\n",
    "            start_index = i\n",
    "            end_index = i + len(wordpieces) - 1\n",
    "#             print(start_index, end_index)\n",
    "            return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no context pooled\n",
    "def sentence_embedding(template, word):\n",
    "    template = \"_\"\n",
    "    sentence = template.replace(\"_\", word)\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = last_hidden_states\n",
    "    input_mask_expanded = inputs.attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    start, end = get_index(sentence, word, tokenizer)\n",
    "    sum_embeddings = torch.sum(token_embeddings[0][start:end], 0)\n",
    "    pooled_output = (sum_embeddings)\n",
    "    return pooled_output.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(x, y):\n",
    "    return np.dot(x, y) / math.sqrt(np.dot(x, x) * np.dot(y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cossim_lookup(XY, AB):\n",
    "    \"\"\"\n",
    "    XY: mapping from target string to target vector (either in X or Y)\n",
    "    AB: mapping from attribute string to attribute vectore (either in A or B)\n",
    "    Returns an array of size (len(XY), len(AB)) containing cosine similarities\n",
    "    between items in XY and items in AB.\n",
    "    \"\"\"\n",
    "\n",
    "    cossims = np.zeros((len(XY), len(AB)))\n",
    "    for xy in XY:\n",
    "        for ab in AB:\n",
    "#             print(XY[xy].shape)\n",
    "#             print(AB[ab].shape)\n",
    "            cossims[xy, ab] = cossim(XY[xy], AB[ab])\n",
    "    return cossims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keys_to_ints(X, Y):\n",
    "    return (\n",
    "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
    "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def s_XAB(A, s_wAB_memo):\n",
    "    return s_wAB_memo[A].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_wAB(X, Y, cossims):\n",
    "    \"\"\"\n",
    "    Return vector of s(w, A, B) across w, where\n",
    "        s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "    \"\"\"\n",
    "#     print((cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)).shape)\n",
    "    return cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def s_wAB_df(X, Y, cossims):\n",
    "#     \"\"\"\n",
    "#     Return vector of s(w, A, B) across w, where\n",
    "#         s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "#     \"\"\"\n",
    "# #     print(cossims[:, A].mean(axis=1))\n",
    "#     df1 = pd.DataFrame(cossims[X, :].mean(axis=0))\n",
    "#     df2 = pd.DataFrame(cossims[Y, :].mean(axis=0))\n",
    "#     return df1, df2\n",
    "\n",
    "\n",
    "def s_XAB_df(A, B, s_wAB_memo):\n",
    "    df1 = pd.DataFrame(s_wAB_memo[A])\n",
    "    df2 = pd.DataFrame(s_wAB_memo[B])\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_XYAB(A, B, s_wAB_memo):\n",
    "    r\"\"\"\n",
    "    Given indices of target concept X and precomputed s_wAB values,\n",
    "    the WEAT test statistic for p-value computation.\n",
    "    \"\"\"\n",
    "    return s_XAB(A, s_wAB_memo) - s_XAB(B, s_wAB_memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def WEAT_test(X, Y, A, B, n_samples, cossims, parametric=False):\n",
    "def WEAT_test(X, Y, A, B, n_samples, cossims):\n",
    "    ''' Compute the p-val for the permutation test, which is defined as\n",
    "        the probability that a random even partition X_i, Y_i of X u Y\n",
    "        satisfies P[s(X_i, Y_i, A, B) > s(X, Y, A, B)]\n",
    "    '''\n",
    "    X = np.array(list(X), dtype=np.int)\n",
    "    Y = np.array(list(Y), dtype=np.int)\n",
    "    A = np.array(list(A), dtype=np.int)\n",
    "    B = np.array(list(B), dtype=np.int)\n",
    "\n",
    "    assert len(X) == len(Y)\n",
    "    size = len(X)\n",
    "    s_wAB_memo = s_wAB(X, Y, cossims=cossims)\n",
    "#     print(s_wAB_memo)\n",
    "    XY = np.concatenate((X, Y))\n",
    "\n",
    "#     if parametric:\n",
    "#     log.info('Using parametric test')\n",
    "    s = s_XYAB(A, B, s_wAB_memo)\n",
    "    return s\n",
    "\n",
    "\n",
    "def convert_keys_to_ints(X, Y):\n",
    "    return (\n",
    "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
    "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(df1, df2, k=0):\n",
    "    diff = (df1[k].mean() - df2[k].mean())\n",
    "    std_ = pd.concat([df1, df2], axis=0)[k].std() + 1e-8\n",
    "    return diff / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n",
      "0.9333309039803412\n",
      "0.7172162185504821\n",
      "1.2683785663996838\n",
      "0.5124438567798357\n",
      "1.3139395873111677\n",
      "0.3572137331006403\n",
      "0.5612173102647714\n",
      "1.1439391162110863\n",
      "0.522983855749374\n",
      "0.8210941166613612\n",
      "0.6417161652623002\n",
      "0.8639103766989579\n",
      "0.9042815630991883\n",
      "0.833185021869976\n",
      "0.5857613963530481\n",
      "0.6671838420511793\n",
      "0.24836739430710353\n",
      "0.7533510038782\n",
      "0.8589660760433101\n",
      "0.46748390858399486\n"
     ]
    }
   ],
   "source": [
    "template_score_dict = {}\n",
    "for ind, template in enumerate(templates):\n",
    "    score_dict = {}\n",
    "    attribute_template = template\n",
    "    target_template = template\n",
    "#     for model_ind in range(len(models)):\n",
    "#     for model_ind in [2]:\n",
    "    X = {\"x\" + str(j): sentence_embedding(attribute_template, j) for j in XX}\n",
    "    Y = {\"y\" + str(j): sentence_embedding(attribute_template, j) for j in YY}\n",
    "    (X, Y) = convert_keys_to_ints(X, Y)\n",
    "    XY = X.copy()\n",
    "    XY.update(Y)\n",
    "    X = np.array(list(X), dtype=np.int)\n",
    "    Y = np.array(list(Y), dtype=np.int)\n",
    "    for i in range(len(female_list)):\n",
    "        AA = female_list[i]\n",
    "    #     print(AA)\n",
    "    #     print(XX)\n",
    "        BB = male_list[i]\n",
    "\n",
    "        A = {\"a\" + str(j): sentence_embedding(target_template, j) for j in AA}\n",
    "        B = {\"b\" + str(j): sentence_embedding(target_template, j) for j in BB}\n",
    "\n",
    "        (A, B) = convert_keys_to_ints(A, B)\n",
    "\n",
    "\n",
    "        AB = A.copy()\n",
    "        AB.update(B)\n",
    "\n",
    "        cossims = construct_cossim_lookup(XY, AB)\n",
    "        A = np.array(list(A), dtype=np.int)\n",
    "        B = np.array(list(B), dtype=np.int)\n",
    "\n",
    "\n",
    "        s_wAB_memo = s_wAB(X, Y, cossims=cossims)\n",
    "        df1,df2 = s_XAB_df(A, B, s_wAB_memo)\n",
    "        effect_size = get_effect_size(df1, df2)\n",
    "        score_dict[i] = effect_size\n",
    "        print(score_dict[i])\n",
    "    template_score_dict[temps[ind]] = score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a_file = open(\"3_seat_no_context_pooled_embeddings.json\", \"w\")\n",
    "json.dump(template_score_dict, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
