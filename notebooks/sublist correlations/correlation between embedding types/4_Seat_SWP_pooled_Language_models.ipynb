{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_file\n",
    "male_list = data_file.male_list\n",
    "female_list = data_file.female_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gendered_word_list\n",
    "# gendered_word_list = [[\"she\", \"he\"],[\"female\", \"male\"], [\"woman\", \"man\"], [\"girl\", \"boy\"], [\"sister\", \"brother\"], [\"daughter\", \"son\"], [\"mother\", \"father\"]]\n",
    "XX = [\"female\", \"woman\", \"girl\", \"sister\", \"daughter\", \"mother\", \"aunt\", \"grandmother\", \"daughter\"]\n",
    "YY = [\"male\", \"man\", \"boy\", \"brother\", \"son\", \"father\", \"uncle\", \"grandfather\", \"son\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\"this is the _.\", \"that is the _.\", \"there is the _.\", \"here is the _.\", \"the _ is here.\", \"the _ is there.\", \"the _ is a person.\", \"it is the _.\",\"the _ is a [MASK].\", \"the _ is an engineer.\", \"the _ is a nurse with superior technical skills.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [\"temp1\", \"temp2\", \"temp3\", \"temp4\",\"temp5\", \"temp6\", \"temp7\", \"temp8\", \"temp9\", \"temp10\", \"temp11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(sentence, wordd):\n",
    "    toks = tokenizer(sentence).input_ids\n",
    "    wordpieces = tokenizer(wordd).input_ids\n",
    "    word = wordpieces[1] # use first wordpiece\n",
    "    for i, t in enumerate(toks):\n",
    "        if t == word:\n",
    "            start_index = i\n",
    "            end_index = i + len(wordpieces) - 1\n",
    "#             print(start_index, end_index)\n",
    "            return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWP pooled\n",
    "def sentence_embedding(template, word):\n",
    "    sentence = template.replace(\"_\", word)\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = last_hidden_states\n",
    "    input_mask_expanded = inputs.attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    start, end = get_index(sentence, word)\n",
    "    sum_embeddings = torch.sum(token_embeddings[0][start:end], 0)\n",
    "    pooled_output = (sum_embeddings)\n",
    "    return pooled_output.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(x, y):\n",
    "    return np.dot(x, y) / math.sqrt(np.dot(x, x) * np.dot(y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cossim_lookup(XY, AB):\n",
    "    \"\"\"\n",
    "    XY: mapping from target string to target vector (either in X or Y)\n",
    "    AB: mapping from attribute string to attribute vectore (either in A or B)\n",
    "    Returns an array of size (len(XY), len(AB)) containing cosine similarities\n",
    "    between items in XY and items in AB.\n",
    "    \"\"\"\n",
    "\n",
    "    cossims = np.zeros((len(XY), len(AB)))\n",
    "    for xy in XY:\n",
    "        for ab in AB:\n",
    "#             print(XY[xy].shape)\n",
    "#             print(AB[ab].shape)\n",
    "            cossims[xy, ab] = cossim(XY[xy], AB[ab])\n",
    "    return cossims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keys_to_ints(X, Y):\n",
    "    return (\n",
    "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
    "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def s_XAB(A, s_wAB_memo):\n",
    "    return s_wAB_memo[A].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_wAB(X, Y, cossims):\n",
    "    \"\"\"\n",
    "    Return vector of s(w, A, B) across w, where\n",
    "        s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "    \"\"\"\n",
    "#     print((cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)).shape)\n",
    "    return cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def s_wAB_df(X, Y, cossims):\n",
    "#     \"\"\"\n",
    "#     Return vector of s(w, A, B) across w, where\n",
    "#         s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "#     \"\"\"\n",
    "# #     print(cossims[:, A].mean(axis=1))\n",
    "#     df1 = pd.DataFrame(cossims[X, :].mean(axis=0))\n",
    "#     df2 = pd.DataFrame(cossims[Y, :].mean(axis=0))\n",
    "#     return df1, df2\n",
    "\n",
    "\n",
    "def s_XAB_df(A, B, s_wAB_memo):\n",
    "    df1 = pd.DataFrame(s_wAB_memo[A])\n",
    "    df2 = pd.DataFrame(s_wAB_memo[B])\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_XYAB(A, B, s_wAB_memo):\n",
    "    r\"\"\"\n",
    "    Given indices of target concept X and precomputed s_wAB values,\n",
    "    the WEAT test statistic for p-value computation.\n",
    "    \"\"\"\n",
    "    return s_XAB(A, s_wAB_memo) - s_XAB(B, s_wAB_memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def WEAT_test(X, Y, A, B, n_samples, cossims, parametric=False):\n",
    "def WEAT_test(X, Y, A, B, n_samples, cossims):\n",
    "    ''' Compute the p-val for the permutation test, which is defined as\n",
    "        the probability that a random even partition X_i, Y_i of X u Y\n",
    "        satisfies P[s(X_i, Y_i, A, B) > s(X, Y, A, B)]\n",
    "    '''\n",
    "    X = np.array(list(X), dtype=np.int)\n",
    "    Y = np.array(list(Y), dtype=np.int)\n",
    "    A = np.array(list(A), dtype=np.int)\n",
    "    B = np.array(list(B), dtype=np.int)\n",
    "\n",
    "    assert len(X) == len(Y)\n",
    "    size = len(X)\n",
    "    s_wAB_memo = s_wAB(X, Y, cossims=cossims)\n",
    "#     print(s_wAB_memo)\n",
    "    XY = np.concatenate((X, Y))\n",
    "\n",
    "#     if parametric:\n",
    "#     log.info('Using parametric test')\n",
    "    s = s_XYAB(A, B, s_wAB_memo)\n",
    "    return s\n",
    "\n",
    "\n",
    "def convert_keys_to_ints(X, Y):\n",
    "    return (\n",
    "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
    "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(df1, df2, k=0):\n",
    "    diff = (df1[k].mean() - df2[k].mean())\n",
    "    std_ = pd.concat([df1, df2], axis=0)[k].std() + 1e-8\n",
    "    return diff / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82715737727093\n",
      "0.8669357083243805\n",
      "1.2260823680569828\n",
      "0.7808005460154708\n",
      "1.192914159325011\n",
      "0.9871822553931009\n",
      "0.7210586659404039\n",
      "1.1103776361608908\n",
      "0.7836038677875609\n",
      "1.1934771356653568\n",
      "1.0182494227270684\n",
      "1.003401578359237\n",
      "1.0681670087543444\n",
      "1.1255392690656965\n",
      "0.8459489589255266\n",
      "0.7850663110529585\n",
      "0.6971930084739035\n",
      "0.9310150756840101\n",
      "0.6870967592219381\n",
      "0.3434783476439827\n",
      "0.6266196193222822\n",
      "0.7535363982413533\n",
      "1.2385262701320947\n",
      "0.7012890353633108\n",
      "1.3248977746624613\n",
      "0.9728365044000229\n",
      "0.945460265561184\n",
      "1.1797258064519518\n",
      "0.8334506355815201\n",
      "1.115977396643085\n",
      "0.9728859917780271\n",
      "0.9990967280430622\n",
      "1.15638947222134\n",
      "1.0101928083440543\n",
      "0.7889568034239826\n",
      "0.6612977618146987\n",
      "0.7806796812385313\n",
      "0.9216680836719915\n",
      "0.8087297220835411\n",
      "0.6002173488161295\n",
      "0.5893699948497048\n",
      "0.7752293152893326\n",
      "0.9095228020438797\n",
      "0.5768815794972384\n",
      "0.6563129172881331\n",
      "1.117591144586323\n",
      "0.8140817772666464\n",
      "1.0520045347684663\n",
      "0.8243086988018061\n",
      "0.8719504208801842\n",
      "0.9001884339627603\n",
      "0.4716750178494203\n",
      "0.6938631384627878\n",
      "0.8875339078009811\n",
      "0.7146704911347682\n",
      "1.1248744257459256\n",
      "0.6735869804496659\n",
      "0.9866349299070584\n",
      "0.8757758578016391\n",
      "0.5642129473117464\n",
      "1.1201306185978797\n",
      "1.2279885176519616\n",
      "1.2308111609454624\n",
      "0.9446378271181978\n",
      "1.2355503357226596\n",
      "1.5346651311163781\n",
      "0.764375894148805\n",
      "1.2853233304183573\n",
      "1.2290055292590112\n",
      "1.2735821606489035\n",
      "0.8061993440979381\n",
      "1.0655782904582363\n",
      "1.1230536246626002\n",
      "0.7596089265991434\n",
      "1.2559150837042283\n",
      "1.1694507838486818\n",
      "1.0825292848769645\n",
      "1.310155203121232\n",
      "1.0739025117694494\n",
      "1.12548516625956\n",
      "1.028277008282521\n",
      "0.8917003277883196\n",
      "1.1032182721985686\n",
      "0.9879524735414376\n",
      "1.2155910122505602\n",
      "1.223642412628854\n",
      "0.9397305386567075\n",
      "1.0009856408700997\n",
      "0.8805590825300339\n",
      "0.8237398503255146\n",
      "0.7548962705639729\n",
      "0.8482550119181702\n",
      "1.0106845370256026\n",
      "0.8691272664577747\n",
      "1.0298828975500396\n",
      "0.5645992367947283\n",
      "1.2396406307476762\n",
      "0.6988916093324179\n",
      "0.8403263882282089\n",
      "0.34050337052942525\n",
      "0.9873832865049105\n",
      "0.8341623174039028\n",
      "1.2315944997971315\n",
      "0.9107741845397526\n",
      "1.248078932025934\n",
      "0.9478776213476907\n",
      "0.9143146323005824\n",
      "1.016231028815313\n",
      "0.6260660054932561\n",
      "0.8457822490583174\n",
      "0.7800898561808027\n",
      "0.9190129398727331\n",
      "1.0364422908680406\n",
      "0.8713494215519421\n",
      "0.9741759704956057\n",
      "0.36377325246587766\n",
      "1.0996168450455897\n",
      "0.6603121044803945\n",
      "0.7933071348318526\n",
      "0.3791407328392241\n",
      "0.4264167421928002\n",
      "0.6021853164074564\n",
      "1.172633563006106\n",
      "0.3241352624496751\n",
      "1.110506377281963\n",
      "0.273550068565161\n",
      "0.4845295777019912\n",
      "0.6552419881695721\n",
      "0.4597508822065636\n",
      "0.74543281283956\n",
      "0.8323782302903582\n",
      "0.9879624542767136\n",
      "0.7483612949410213\n",
      "0.9443772869095199\n",
      "0.30498558958871697\n",
      "0.06435029830901237\n",
      "0.35726737938178144\n",
      "0.8192538012674109\n",
      "0.4772117053990876\n",
      "0.15489981573169284\n",
      "1.2399003706203207\n",
      "0.9814178824725617\n",
      "1.3529762404557824\n",
      "1.0163808587616623\n",
      "1.0919274007920685\n",
      "1.583749539661556\n",
      "0.8662293747476464\n",
      "1.2913366971243923\n",
      "1.0707159816630591\n",
      "1.254386483022006\n",
      "0.9746043998229136\n",
      "0.9724871103431237\n",
      "1.018859903725657\n",
      "1.260030831914192\n",
      "1.2598980254268044\n",
      "1.163352558383707\n",
      "1.0835023180577816\n",
      "0.8568688696339152\n",
      "1.17306816495144\n",
      "0.7472581108715858\n",
      "0.8333051496872871\n",
      "0.9876594329499211\n",
      "1.4165631320538647\n",
      "0.5942649916750403\n",
      "1.3431843145693823\n",
      "0.7452885834948754\n",
      "0.595552393653248\n",
      "0.9994001654714363\n",
      "0.6863737676623811\n",
      "1.2396782690673789\n",
      "0.8496469248514872\n",
      "1.061622311660676\n",
      "0.9452272565591007\n",
      "0.9373658068997379\n",
      "1.0067589446903364\n",
      "0.7690853897725233\n",
      "0.7611687481814966\n",
      "1.1007149413851336\n",
      "0.7695793557679863\n",
      "0.8656903683881989\n",
      "0.5876771795583524\n",
      "0.49840700131988797\n",
      "0.9426002059146327\n",
      "0.25748376150238134\n",
      "1.0990726015293741\n",
      "0.37485938444588873\n",
      "-0.02045287269117433\n",
      "0.5684024667695564\n",
      "0.6153731535174143\n",
      "0.4873624974299829\n",
      "0.44597756996680965\n",
      "0.8006680573127917\n",
      "0.5117944298937444\n",
      "0.16467196083746197\n",
      "0.4406989666762839\n",
      "0.41285301965027105\n",
      "0.6287438961156635\n",
      "0.4732218165866462\n",
      "0.7583934886630245\n",
      "0.4683035704877919\n",
      "0.867479486457457\n",
      "1.013898788185495\n",
      "1.0962305311751315\n",
      "0.2686780660445873\n",
      "1.2329039317981003\n",
      "0.0951057979529853\n",
      "0.4190017689813033\n",
      "0.6263407755763801\n",
      "0.8595254858736723\n",
      "0.8189886940460371\n",
      "0.7077442745164085\n",
      "0.9235973265027378\n",
      "0.49753997564520347\n",
      "0.5221978076585774\n",
      "0.78598829854279\n",
      "0.9242467029916717\n",
      "0.7420807071486052\n",
      "0.8475374715620246\n",
      "0.8890501449676322\n",
      "0.7641552896895648\n"
     ]
    }
   ],
   "source": [
    "template_score_dict = {}\n",
    "for ind, template in enumerate(templates):\n",
    "    score_dict = {}\n",
    "    attribute_template = template\n",
    "    target_template = template\n",
    "#     for model_ind in range(len(models)):\n",
    "#     for model_ind in [2]:\n",
    "    X = {\"x\" + str(j): sentence_embedding(attribute_template, j) for j in XX}\n",
    "    Y = {\"y\" + str(j): sentence_embedding(attribute_template, j) for j in YY}\n",
    "    (X, Y) = convert_keys_to_ints(X, Y)\n",
    "    XY = X.copy()\n",
    "    XY.update(Y)\n",
    "    X = np.array(list(X), dtype=np.int)\n",
    "    Y = np.array(list(Y), dtype=np.int)\n",
    "    for i in range(len(female_list)):\n",
    "        AA = female_list[i]\n",
    "    #     print(AA)\n",
    "    #     print(XX)\n",
    "        BB = male_list[i]\n",
    "\n",
    "        A = {\"a\" + str(j): sentence_embedding(target_template, j) for j in AA}\n",
    "        B = {\"b\" + str(j): sentence_embedding(target_template, j) for j in BB}\n",
    "\n",
    "        (A, B) = convert_keys_to_ints(A, B)\n",
    "\n",
    "\n",
    "        AB = A.copy()\n",
    "        AB.update(B)\n",
    "\n",
    "        cossims = construct_cossim_lookup(XY, AB)\n",
    "        A = np.array(list(A), dtype=np.int)\n",
    "        B = np.array(list(B), dtype=np.int)\n",
    "\n",
    "\n",
    "        s_wAB_memo = s_wAB(X, Y, cossims=cossims)\n",
    "        df1,df2 = s_XAB_df(A, B, s_wAB_memo)\n",
    "        effect_size = get_effect_size(df1, df2)\n",
    "        score_dict[i] = effect_size\n",
    "        print(score_dict[i])\n",
    "    template_score_dict[temps[ind]] = score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a_file = open(\"4_seat_SWP_pooled_embeddings.json\", \"w\")\n",
    "json.dump(template_score_dict, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
