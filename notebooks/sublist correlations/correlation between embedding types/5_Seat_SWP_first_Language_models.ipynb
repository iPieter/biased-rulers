{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_file\n",
    "male_list = data_file.male_list\n",
    "female_list = data_file.female_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gendered_word_list\n",
    "# gendered_word_list = [[\"she\", \"he\"],[\"female\", \"male\"], [\"woman\", \"man\"], [\"girl\", \"boy\"], [\"sister\", \"brother\"], [\"daughter\", \"son\"], [\"mother\", \"father\"]]\n",
    "XX = [\"female\", \"woman\", \"girl\", \"sister\", \"daughter\", \"mother\", \"aunt\", \"grandmother\", \"daughter\"]\n",
    "YY = [\"male\", \"man\", \"boy\", \"brother\", \"son\", \"father\", \"uncle\", \"grandfather\", \"son\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\"this is the _.\", \"that is the _.\", \"there is the _.\", \"here is the _.\", \"the _ is here.\", \"the _ is there.\", \"the _ is a person.\", \"it is the _.\",\"the _ is a [MASK].\", \"the _ is an engineer.\", \"the _ is a nurse with superior technical skills.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [\"temp1\", \"temp2\", \"temp3\", \"temp4\",\"temp5\", \"temp6\", \"temp7\", \"temp8\", \"temp9\", \"temp10\", \"temp11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(sentence, word):\n",
    "    toks = tokenizer(sentence).input_ids\n",
    "    wordpieces = tokenizer(word).input_ids\n",
    "    word = wordpieces[1] # use first wordpiece\n",
    "    for i, t in enumerate(toks):\n",
    "        if t == word:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWP first embedding\n",
    "def sentence_embedding(template, word):\n",
    "    sentence = template.replace(\"_\", word)\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = last_hidden_states\n",
    "    input_mask_expanded = inputs.attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    start = get_index(sentence, word)\n",
    "#     print(start)\n",
    "    embeddings = token_embeddings[0][start]\n",
    "#     print(embeddings.shape)\n",
    "    return embeddings.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# # CLS embedding\n",
    "# def sentence_embedding(template, word, model_ind):\n",
    "#     sentence = template.replace(\"_\", word)\n",
    "#     inputs = tokenizers[model_ind](sentence, return_tensors=\"pt\")\n",
    "#     outputs = models[model_ind](**inputs)\n",
    "#     last_hidden_states = outputs.last_hidden_state\n",
    "#     token_embeddings = last_hidden_states\n",
    "#     return token_embeddings[0][0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(x, y):\n",
    "    return np.dot(x, y) / math.sqrt(np.dot(x, x) * np.dot(y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cossim_lookup(XY, AB):\n",
    "    \"\"\"\n",
    "    XY: mapping from target string to target vector (either in X or Y)\n",
    "    AB: mapping from attribute string to attribute vectore (either in A or B)\n",
    "    Returns an array of size (len(XY), len(AB)) containing cosine similarities\n",
    "    between items in XY and items in AB.\n",
    "    \"\"\"\n",
    "\n",
    "    cossims = np.zeros((len(XY), len(AB)))\n",
    "    for xy in XY:\n",
    "        for ab in AB:\n",
    "#             print(XY[xy].shape)\n",
    "#             print(AB[ab].shape)\n",
    "            cossims[xy, ab] = cossim(XY[xy], AB[ab])\n",
    "    return cossims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keys_to_ints(X, Y):\n",
    "    return (\n",
    "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
    "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def s_XAB(A, s_wAB_memo):\n",
    "    return s_wAB_memo[A].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_wAB(X, Y, cossims):\n",
    "    \"\"\"\n",
    "    Return vector of s(w, A, B) across w, where\n",
    "        s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "    \"\"\"\n",
    "#     print((cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)).shape)\n",
    "    return cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def s_wAB_df(X, Y, cossims):\n",
    "#     \"\"\"\n",
    "#     Return vector of s(w, A, B) across w, where\n",
    "#         s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "#     \"\"\"\n",
    "# #     print(cossims[:, A].mean(axis=1))\n",
    "#     df1 = pd.DataFrame(cossims[X, :].mean(axis=0))\n",
    "#     df2 = pd.DataFrame(cossims[Y, :].mean(axis=0))\n",
    "#     return df1, df2\n",
    "\n",
    "\n",
    "def s_XAB_df(A, B, s_wAB_memo):\n",
    "    df1 = pd.DataFrame(s_wAB_memo[A])\n",
    "    df2 = pd.DataFrame(s_wAB_memo[B])\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_XYAB(A, B, s_wAB_memo):\n",
    "    r\"\"\"\n",
    "    Given indices of target concept X and precomputed s_wAB values,\n",
    "    the WEAT test statistic for p-value computation.\n",
    "    \"\"\"\n",
    "    return s_XAB(A, s_wAB_memo) - s_XAB(B, s_wAB_memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(df1, df2, k=0):\n",
    "    diff = (df1[k].mean() - df2[k].mean())\n",
    "    std_ = pd.concat([df1, df2], axis=0)[k].std() + 1e-8\n",
    "    return diff / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5990485031897409\n",
      "0.6714068637301815\n",
      "1.1631737950151444\n",
      "0.4300088064734563\n",
      "1.0807484633410291\n",
      "0.728444433582044\n",
      "0.5967151799449145\n",
      "0.9699216422992344\n",
      "0.6764622523800463\n",
      "1.0017584512258777\n",
      "0.9014876829399008\n",
      "0.9651974526786529\n",
      "0.9453429085731987\n",
      "0.8318776437875482\n",
      "0.6902093367385614\n",
      "0.5537827970200967\n",
      "0.7582517880853172\n",
      "0.6479462511409895\n",
      "0.5757022345892697\n",
      "0.3803179553070429\n",
      "0.5165489614260299\n",
      "0.6141997131591962\n",
      "1.2558204830200008\n",
      "0.3195518057342596\n",
      "1.085944813136749\n",
      "0.6351491168006498\n",
      "0.48156724495854947\n",
      "0.9409978670467799\n",
      "0.5937688704144524\n",
      "1.0384432796198535\n",
      "0.8576356289772351\n",
      "0.9009797381938763\n",
      "0.8539576566630959\n",
      "0.8045660131944413\n",
      "0.619059379006087\n",
      "0.5190840718699152\n",
      "0.6637209690199731\n",
      "0.6769517532374608\n",
      "0.5767742268934358\n",
      "0.4241470866271133\n",
      "0.5425800457422781\n",
      "0.5389729441356057\n",
      "1.258937457922151\n",
      "0.020865742453891768\n",
      "0.9988570922077628\n",
      "0.24451862086510393\n",
      "0.44158565057387833\n",
      "0.70134961837719\n",
      "0.18831162934032222\n",
      "1.2223636391188837\n",
      "0.6044390822436191\n",
      "0.9361498226970248\n",
      "0.7180995861785867\n",
      "0.7302839503794452\n",
      "0.4403751305777992\n",
      "0.27363360348328897\n",
      "0.56982856816605\n",
      "0.6745865370383084\n",
      "0.33662663450263614\n",
      "0.556867129491973\n",
      "0.6510283531091388\n",
      "0.7188964197089711\n",
      "1.0650880241546745\n",
      "0.48104356973619483\n",
      "1.090780096573657\n",
      "0.7459922251431331\n",
      "0.45169264059344\n",
      "0.8246469637706293\n",
      "0.6024287910897457\n",
      "0.8599545162999112\n",
      "0.6868043901826294\n",
      "0.8481896214006647\n",
      "0.7930951686568611\n",
      "0.6412073786295016\n",
      "0.6739372914312708\n",
      "0.4598187754688528\n",
      "0.8887914357462254\n",
      "0.6199992002480732\n",
      "0.5674801414124733\n",
      "0.3778029393937449\n",
      "0.9895054210858476\n",
      "0.8401734241797896\n",
      "0.9177221088274742\n",
      "0.840196520813888\n",
      "1.154905302714655\n",
      "1.1189406376931637\n",
      "0.7722476978835473\n",
      "0.8918495239330705\n",
      "0.868311736620069\n",
      "0.8138404539177944\n",
      "0.6415637054186752\n",
      "0.7427171202205354\n",
      "0.8685203582901323\n",
      "0.7291208584921108\n",
      "0.9312178369920336\n",
      "0.5808729699704676\n",
      "1.3806713477742663\n",
      "0.47544191204222114\n",
      "0.7474170889029543\n",
      "0.40453741775021157\n",
      "0.8909125123289019\n",
      "0.7487512242729607\n",
      "1.0147710647289583\n",
      "0.7314862899865404\n",
      "1.1586843213891354\n",
      "0.8393994919723062\n",
      "0.7748110504907807\n",
      "0.9138571467143247\n",
      "0.5721634481089002\n",
      "0.8854279699379758\n",
      "0.6134839113612183\n",
      "0.8095865852396023\n",
      "0.8935088981883867\n",
      "0.7428900170102785\n",
      "0.8258569947501844\n",
      "0.4085266493012856\n",
      "1.1862671983513289\n",
      "0.4799152384417512\n",
      "0.62869302169438\n",
      "0.4349923766644631\n",
      "0.34967858847721517\n",
      "0.4634302737271225\n",
      "1.101155498168091\n",
      "0.21745514480026013\n",
      "0.865192890249757\n",
      "0.10313506259277753\n",
      "0.2208359156532461\n",
      "0.42437264014700005\n",
      "0.1791638631620387\n",
      "0.8598103668091744\n",
      "0.7157826884936017\n",
      "0.8825760208772079\n",
      "0.5920584473275682\n",
      "0.9286191046728821\n",
      "0.22650631863340823\n",
      "0.03375424198351976\n",
      "0.40480455257852177\n",
      "0.6083736445375875\n",
      "0.07475778996668267\n",
      "0.04642118939257747\n",
      "0.8254161770407862\n",
      "0.8404224796917055\n",
      "1.2338943741522461\n",
      "0.6348193439956601\n",
      "1.1743154406401417\n",
      "1.049556095602029\n",
      "0.9016557551401224\n",
      "1.083028055335932\n",
      "0.9229232568305623\n",
      "1.1837254875450378\n",
      "0.8618071991451385\n",
      "1.0853551949975508\n",
      "0.9122746001335251\n",
      "0.9818212109881581\n",
      "0.9899778080131086\n",
      "0.6785750477078948\n",
      "1.0504770137090318\n",
      "0.7029627984023545\n",
      "0.8148383578624276\n",
      "0.7428423113640429\n",
      "0.6994446985295769\n",
      "0.7384750524752982\n",
      "1.3182524541195735\n",
      "0.2932643313661639\n",
      "1.043371005192917\n",
      "0.5664796161569426\n",
      "0.4547529811572777\n",
      "0.622927776862874\n",
      "0.5523513976383637\n",
      "1.368388524667467\n",
      "0.8415778677977312\n",
      "0.9821665367905613\n",
      "0.7134467363641889\n",
      "0.8549466936035062\n",
      "0.5801722103237859\n",
      "0.5943195089289884\n",
      "0.6902329922649477\n",
      "1.023429364330191\n",
      "0.4353498659712012\n",
      "0.5989293325102802\n",
      "0.46643627262254156\n",
      "0.40226994916539255\n",
      "1.0677808795695207\n",
      "0.16050890063158169\n",
      "0.9656000461915321\n",
      "0.02628696991949745\n",
      "0.0005032784052173535\n",
      "0.5156103618783696\n",
      "0.22973478050611926\n",
      "0.5970218377608819\n",
      "0.3646647792020731\n",
      "0.7882713771940908\n",
      "0.5046744223380919\n",
      "0.18066711761456697\n",
      "0.35460133291672935\n",
      "0.45326142745844616\n",
      "0.33048209438724835\n",
      "0.6207728830571836\n",
      "0.4068175097144658\n",
      "0.5657656412383525\n",
      "0.7675180318276699\n",
      "0.7472438522388509\n",
      "0.828312862580309\n",
      "0.4168686273069756\n",
      "0.8326566711698874\n",
      "0.10590929890005098\n",
      "-0.022617987917210487\n",
      "0.4291235279292054\n",
      "0.5103200486110087\n",
      "0.646778614030531\n",
      "0.5362835242797857\n",
      "0.675392627607219\n",
      "0.4071493828118769\n",
      "0.3956833153628716\n",
      "0.40633119349242786\n",
      "0.635458407763859\n",
      "0.5584841419681248\n",
      "0.794714672657063\n",
      "0.45984266083322156\n",
      "0.24681417545255638\n"
     ]
    }
   ],
   "source": [
    "template_score_dict = {}\n",
    "for ind, template in enumerate(templates):\n",
    "    score_dict = {}\n",
    "    attribute_template = template\n",
    "    target_template = template\n",
    "#     for model_ind in range(len(models)):\n",
    "#     for model_ind in [2]:\n",
    "    X = {\"x\" + str(j): sentence_embedding(attribute_template, j) for j in XX}\n",
    "    Y = {\"y\" + str(j): sentence_embedding(attribute_template, j) for j in YY}\n",
    "    (X, Y) = convert_keys_to_ints(X, Y)\n",
    "    XY = X.copy()\n",
    "    XY.update(Y)\n",
    "    X = np.array(list(X), dtype=np.int)\n",
    "    Y = np.array(list(Y), dtype=np.int)\n",
    "    for i in range(len(female_list)):\n",
    "        AA = female_list[i]\n",
    "    #     print(AA)\n",
    "    #     print(XX)\n",
    "        BB = male_list[i]\n",
    "\n",
    "        A = {\"a\" + str(j): sentence_embedding(target_template, j) for j in AA}\n",
    "        B = {\"b\" + str(j): sentence_embedding(target_template, j) for j in BB}\n",
    "\n",
    "        (A, B) = convert_keys_to_ints(A, B)\n",
    "\n",
    "\n",
    "        AB = A.copy()\n",
    "        AB.update(B)\n",
    "\n",
    "        cossims = construct_cossim_lookup(XY, AB)\n",
    "        A = np.array(list(A), dtype=np.int)\n",
    "        B = np.array(list(B), dtype=np.int)\n",
    "\n",
    "\n",
    "        s_wAB_memo = s_wAB(X, Y, cossims=cossims)\n",
    "        df1,df2 = s_XAB_df(A, B, s_wAB_memo)\n",
    "        effect_size = get_effect_size(df1, df2)\n",
    "        score_dict[i] = effect_size\n",
    "        print(score_dict[i])\n",
    "    template_score_dict[temps[ind]] = score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a_file = open(\"5_seat_SWP_first_embeddings.json\", \"w\")\n",
    "json.dump(template_score_dict, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
