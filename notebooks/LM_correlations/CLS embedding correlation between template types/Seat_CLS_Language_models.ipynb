{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_file\n",
    "# male_list = data_file.male_list\n",
    "# female_list = data_file.female_list\n",
    "female_professions = data_file.female_professions\n",
    "male_professions = data_file.male_professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "\n",
    "# model1\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "tokenizer1 = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model1 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# model1.cuda()\n",
    "\n",
    "# model2\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "tokenizer2 = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model2 = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "# model2.cuda()\n",
    "\n",
    "# model3\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "tokenizer3 = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model3 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "# model3.cuda()\n",
    "\n",
    "# model4\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "tokenizer4 = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model4 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "# model4.cuda()\n",
    "\n",
    "# model5\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "tokenizer5 = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model5 = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "# model5.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model1, model2, model3, model4, model5]\n",
    "tokenizers = [tokenizer1, tokenizer2, tokenizer3, tokenizer4, tokenizer5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_list = list(female_professions.label1)\n",
    "male_list = list(male_professions.label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gendered_word_list\n",
    "# gendered_word_list = [[\"she\", \"he\"],[\"female\", \"male\"], [\"woman\", \"man\"], [\"girl\", \"boy\"], [\"sister\", \"brother\"], [\"daughter\", \"son\"], [\"mother\", \"father\"]]\n",
    "XX = [\"female\", \"woman\", \"girl\", \"sister\", \"daughter\", \"mother\", \"aunt\", \"grandmother\", \"daughter\"]\n",
    "YY = [\"male\", \"man\", \"boy\", \"brother\", \"son\", \"father\", \"uncle\", \"grandfather\", \"son\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\"this is the _.\", \"that is the _.\", \"there is the _.\", \"here is the _.\", \"the _ is here.\", \"the _ is there.\", \"the _ is a person.\", \"it is the _.\",\"the _ is a [MASK].\", \"the _ is an engineer.\", \"the _ is a nurse with superior technical skills.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [\"temp1\", \"temp2\", \"temp3\", \"temp4\",\"temp5\", \"temp6\", \"temp7\", \"temp8\", \"temp9\", \"temp10\", \"temp11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS embedding\n",
    "def sentence_embedding(template, word, model_ind):\n",
    "    sentence = template.replace(\"_\", word)\n",
    "    inputs = tokenizers[model_ind](sentence, return_tensors=\"pt\")\n",
    "    outputs = models[model_ind](**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = last_hidden_states\n",
    "    return token_embeddings[0][0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(x, y):\n",
    "    return np.dot(x, y) / math.sqrt(np.dot(x, x) * np.dot(y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cossim_lookup(XY, AB):\n",
    "    \"\"\"\n",
    "    XY: mapping from target string to target vector (either in X or Y)\n",
    "    AB: mapping from attribute string to attribute vectore (either in A or B)\n",
    "    Returns an array of size (len(XY), len(AB)) containing cosine similarities\n",
    "    between items in XY and items in AB.\n",
    "    \"\"\"\n",
    "\n",
    "    cossims = np.zeros((len(XY), len(AB)))\n",
    "    for xy in XY:\n",
    "        for ab in AB:\n",
    "            cossims[xy, ab] = cossim(XY[xy], AB[ab])\n",
    "    return cossims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keys_to_ints(X, Y):\n",
    "    return (\n",
    "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
    "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def s_XAB(A, s_wAB_memo):\n",
    "    return s_wAB_memo[A].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_wAB(X, Y, cossims):\n",
    "    \"\"\"\n",
    "    Return vector of s(w, A, B) across w, where\n",
    "        s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "    \"\"\"\n",
    "#     print((cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)).shape)\n",
    "    return cossims[X, :].mean(axis=0) - cossims[Y, :].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def s_wAB_df(X, Y, cossims):\n",
    "#     \"\"\"\n",
    "#     Return vector of s(w, A, B) across w, where\n",
    "#         s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
    "#     \"\"\"\n",
    "# #     print(cossims[:, A].mean(axis=1))\n",
    "#     df1 = pd.DataFrame(cossims[X, :].mean(axis=0))\n",
    "#     df2 = pd.DataFrame(cossims[Y, :].mean(axis=0))\n",
    "#     return df1, df2\n",
    "\n",
    "\n",
    "def s_XAB_df(A, B, s_wAB_memo):\n",
    "    df1 = pd.DataFrame(s_wAB_memo[A])\n",
    "    df2 = pd.DataFrame(s_wAB_memo[B])\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_XYAB(A, B, s_wAB_memo):\n",
    "    r\"\"\"\n",
    "    Given indices of target concept X and precomputed s_wAB values,\n",
    "    the WEAT test statistic for p-value computation.\n",
    "    \"\"\"\n",
    "    return s_XAB(A, s_wAB_memo) - s_XAB(B, s_wAB_memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def WEAT_test(X, Y, A, B, n_samples, cossims, parametric=False):\n",
    "def WEAT_test(X, Y, A, B, n_samples, cossims):\n",
    "    ''' Compute the p-val for the permutation test, which is defined as\n",
    "        the probability that a random even partition X_i, Y_i of X u Y\n",
    "        satisfies P[s(X_i, Y_i, A, B) > s(X, Y, A, B)]\n",
    "    '''\n",
    "    X = np.array(list(X), dtype=np.int)\n",
    "    Y = np.array(list(Y), dtype=np.int)\n",
    "    A = np.array(list(A), dtype=np.int)\n",
    "    B = np.array(list(B), dtype=np.int)\n",
    "\n",
    "    assert len(X) == len(Y)\n",
    "    size = len(X)\n",
    "    s_wAB_memo = s_wAB(X, Y, cossims=cossims)\n",
    "#     print(s_wAB_memo)\n",
    "    XY = np.concatenate((X, Y))\n",
    "\n",
    "#     if parametric:\n",
    "#     log.info('Using parametric test')\n",
    "    s = s_XYAB(A, B, s_wAB_memo)\n",
    "    return s\n",
    "\n",
    "\n",
    "def convert_keys_to_ints(X, Y):\n",
    "    return (\n",
    "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
    "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(df1, df2, k=0):\n",
    "    diff = (df1[k].mean() - df2[k].mean())\n",
    "    std_ = pd.concat([df1, df2], axis=0)[k].std() + 1e-8\n",
    "    return diff / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.167137765206161\n",
      "0.613181978410407\n",
      "1.2450650026597185\n",
      "0.8241778491485858\n",
      "0.3899740677999021\n",
      "1.1429489272509046\n",
      "0.7682580209621059\n",
      "1.0448924399341275\n",
      "0.871382442573088\n",
      "0.15952797256527132\n",
      "0.6468110250285095\n",
      "0.23944622026258083\n",
      "1.1923981150296132\n",
      "0.41716703855199594\n",
      "0.37244794398602465\n",
      "1.1573514938240603\n",
      "0.5820993930584908\n",
      "1.0615581872798616\n",
      "0.7615882028614193\n",
      "0.7578026564668334\n",
      "0.9910124198202783\n",
      "0.40116426444909276\n",
      "1.1237046148444554\n",
      "0.7650705113511841\n",
      "0.3550183256840251\n",
      "1.0536814040106905\n",
      "0.5898308774572396\n",
      "1.1864499464559901\n",
      "0.8110554706993732\n",
      "0.3941374836136234\n",
      "0.6099462158905813\n",
      "0.3150538995972494\n",
      "1.2670202061475453\n",
      "0.8481557113873969\n",
      "0.3820282424854507\n",
      "1.19501511543039\n",
      "0.7900532245667686\n",
      "1.159718639716848\n",
      "0.7631024037307255\n",
      "0.41156324117274534\n",
      "1.0619592517147627\n",
      "0.4723792698741765\n",
      "1.1050926948724156\n",
      "1.0947279057120534\n",
      "0.16922043676379692\n",
      "0.3759700281077367\n",
      "0.666829483801519\n",
      "1.3187638450729833\n",
      "0.8741564533514405\n",
      "0.5007626048939062\n",
      "0.5334231945190242\n",
      "0.2554062954334604\n",
      "0.9558810376045204\n",
      "0.3856283547668909\n",
      "0.4594229436808875\n"
     ]
    }
   ],
   "source": [
    "template_score_dict = {}\n",
    "for ind, template in enumerate(templates):\n",
    "    score_dict = {}\n",
    "    attribute_template = template\n",
    "    target_template = template\n",
    "    for model_ind in range(len(models)):\n",
    "        X = {\"x\" + str(j): sentence_embedding(attribute_template, j, model_ind) for j in XX}\n",
    "        Y = {\"y\" + str(j): sentence_embedding(attribute_template, j, model_ind) for j in YY}\n",
    "        (X, Y) = convert_keys_to_ints(X, Y)\n",
    "        XY = X.copy()\n",
    "        XY.update(Y)\n",
    "        X = np.array(list(X), dtype=np.int)\n",
    "        Y = np.array(list(Y), dtype=np.int)\n",
    "        AA = female_list\n",
    "    #     print(AA)\n",
    "    #     print(XX)\n",
    "        BB = male_list\n",
    "\n",
    "        A = {\"a\" + str(j): sentence_embedding(target_template, j, model_ind) for j in AA}\n",
    "        B = {\"b\" + str(j): sentence_embedding(target_template, j, model_ind) for j in BB}\n",
    "\n",
    "        (A, B) = convert_keys_to_ints(A, B)\n",
    "\n",
    "\n",
    "        AB = A.copy()\n",
    "        AB.update(B)\n",
    "\n",
    "        cossims = construct_cossim_lookup(XY, AB)\n",
    "        A = np.array(list(A), dtype=np.int)\n",
    "        B = np.array(list(B), dtype=np.int)\n",
    "\n",
    "\n",
    "        s_wAB_memo = s_wAB(X, Y, cossims=cossims)\n",
    "        df1,df2 = s_XAB_df(A, B, s_wAB_memo)\n",
    "        effect_size = get_effect_size(df1, df2)\n",
    "        score_dict[model_ind] = effect_size\n",
    "        print(score_dict[model_ind])\n",
    "    template_score_dict[temps[ind]] = score_dict\n",
    "# log.info(\"pval: %g\", pval)\n",
    "\n",
    "# log.info(\"computing effect size...\")\n",
    "# esize = effect_size(X, Y, A, B, cossims=cossims)\n",
    "# log.info(\"esize: %g\", esize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a_file = open(\"seat_CLS_templates_Language_models.json\", \"w\")\n",
    "json.dump(template_score_dict, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
