{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "from src.metrics import seat, lpbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.2183891801905424, 1: 1.1494166725869472, 2: 1.0876914626353713, 3: 1.1775286878780287, 4: 1.4163213586462502, 5: 1.2950851894951154, 6: 1.2639451503281618, 7: 1.0514229043180452, 8: 1.214565758255554, 9: 1.2051362169611868, 10: 1.0848235989185238, 11: 1.027950708645785, 12: 1.236813556375959, 13: 0.8492048691495063, 14: 1.29217896189363, 15: 1.1385867962846512, 16: 1.185702037906551, 17: 1.0455115821090182, 18: 1.233914191834782, 19: 1.2185116110453875}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attribute_template = \"This is the _.\"\n",
    "target_template = \"This is the _.\"\n",
    "\n",
    "print(seat.seat_test(attribute_template, target_template, tokenizer, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.8712208646044697, 1: 1.058285319386308, 2: 1.3139517070717035, 3: 0.8155197844164715, 4: 1.3221400790178275, 5: 0.9451985484852444, 6: 0.9105467860940959, 7: 1.4382343005457392, 8: 0.9077465293875092, 9: 1.173698640973059, 10: 0.8851261243220466, 11: 1.2028881070832158, 12: 0.9902692981089054, 13: 1.1613303128006802, 14: 1.2099159345422645, 15: 0.7557550213993255, 16: 0.7922075649555417, 17: 1.0890842309441016, 18: 0.88942015184123, 19: 0.8198238074423987}\n"
     ]
    }
   ],
   "source": [
    "print(seat.lauscher_et_al_test(attribute_template, target_template, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.7236307836559362, 1: 0.735665249491224, 2: 1.190387358510162, 3: 0.6466443393922682, 4: 1.2353762496799099, 5: 0.9062731457484468, 6: 0.8668005790616289, 7: 1.021647268318636, 8: 0.7815163111358471, 9: 0.9549401853143625, 10: 0.9440463847337583, 11: 0.9411252904925845, 12: 1.0025084528225345, 13: 0.897598538114084, 14: 0.8395899204861828, 15: 0.5940909068049024, 16: 0.8911453081035888, 17: 0.7167719098601603, 18: 0.8188156742195554, 19: 0.43772511354757954}\n"
     ]
    }
   ],
   "source": [
    "print(seat.tan_et_al_test(attribute_template, target_template, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "results = lpbs.lpbs_test(\"\",\"\", tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7180788398979967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['gender_fill_bias_prior_corrected']['std']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e94ad2f0a869c7a0d4fd2f25322f5fcfa75a94c6b344072802a64a2c890d2c64"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
